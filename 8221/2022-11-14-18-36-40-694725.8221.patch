diff --git a/Makefile.am b/Makefile.am
index a1195f58a..0cd664cfd 100644
--- a/Makefile.am
+++ b/Makefile.am
@@ -163,6 +163,7 @@ src_libfabric_la_SOURCES =			\
 	include/ofi_iov.h			\
 	include/ofi_list.h			\
 	include/ofi_bitmask.h			\
+	include/ofi_atomic_queue.h		\
 	include/shared/ofi_str.h		\
 	include/ofi_lock.h			\
 	include/ofi_mem.h			\
diff --git a/include/ofi_atomic_queue.h b/include/ofi_atomic_queue.h
new file mode 100644
index 000000000..68a8b43ac
--- /dev/null
+++ b/include/ofi_atomic_queue.h
@@ -0,0 +1,186 @@
+/*
+ * Copyright (c) 2022 UT-Battelle ORNL. All rights reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef OFI_ATOMIC_QUEUE_H
+#define OFI_ATOMIC_QUEUE_H
+
+#include <ofi_atom.h>
+
+/*
+ * This is an atomic queue, meaning no need for locking. One example usage
+ * for this data structure is to build a command queue shared between
+ * different processes. Multiple processes would post commands on a command
+ * queue which belongs to another process. The receiving process would
+ * read and process commands off the queue.
+ *
+ * Usage:
+ *  . OFI_DECLARE_ATOMIQ() to declare the atomic queue
+ *  . Allocate shared memory for the queue or call the create method
+ *  . call the init() method to ready the queue for usage
+ *  . To post on the queue call tx_next() method
+ *     . This will return a buffer of entrytype
+ *     . Initialize the entry
+ *  . Call tx_advance() method to post for the reader
+ *  . To read off the queue call rx_next()
+ *     . This will return the next available entry on the queue or
+ *       -FI_ENOENT if there are no more entries on the queue
+ *  . Call the rx_adance() after you've completed using the entry
+ */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#define OFI_CACHE_LINE_SIZE (64)
+
+#define OFI_DECLARE_ATOMIQ(entrytype, name)			\
+struct name ## _entry {						\
+	ofi_atomic64_t	seq;					\
+	entrytype	buf;					\
+};								\
+struct name {							\
+	int		size;					\
+	int		size_mask;				\
+	ofi_atomic64_t	enqueue_pos;				\
+	char		pad0[OFI_CACHE_LINE_SIZE];		\
+	ofi_atomic64_t	dequeue_pos;				\
+	char		pad1[OFI_CACHE_LINE_SIZE];		\
+	struct name ## _entry entry[];				\
+} __attribute__((__aligned__(64)));				\
+								\
+static inline void name ## _init(struct name *aq, size_t size)	\
+{								\
+	size_t i;						\
+	assert(size == roundup_power_of_two(size));		\
+	aq->size = size;					\
+	aq->size_mask = aq->size - 1;				\
+	ofi_atomic_initialize64(&aq->enqueue_pos, 0);		\
+	ofi_atomic_initialize64(&aq->dequeue_pos, 0);		\
+	for (i = 0; i < size; i++)				\
+		ofi_atomic_initialize64(&aq->entry[i].seq, i);	\
+}								\
+								\
+static inline struct name * name ## _create(size_t size)	\
+{								\
+	struct name *aq;					\
+	aq = (struct name*) calloc(1, sizeof(*aq) +		\
+		sizeof(struct name ## _entry) *			\
+		(roundup_power_of_two(size)));			\
+	if (aq)							\
+		name ##_init(aq, roundup_power_of_two(size));	\
+	return aq;						\
+}								\
+								\
+static inline void name ## _free(struct name *aq)		\
+{								\
+	free(aq);						\
+}								\
+static inline int name ## _tx_next(struct name *aq,		\
+		entrytype **buf, int64_t *pos)			\
+{								\
+	struct name ## _entry *e;				\
+	int64_t diff, seq;					\
+	*pos = atomic_load_explicit(&aq->enqueue_pos.val,	\
+				    memory_order_relaxed);	\
+	for (;;) {						\
+		e = &aq->entry[*pos & aq->size_mask];		\
+		seq = atomic_load_explicit(&(e->seq.val),	\
+			memory_order_acquire);			\
+		diff = seq - *pos;				\
+		if (diff == 0) {				\
+			if(atomic_compare_exchange_weak(	\
+				&aq->enqueue_pos.val, pos,	\
+				*pos + 1))			\
+				break;				\
+		} else if (diff < 0) {				\
+			return -FI_EAGAIN;			\
+		} else {					\
+			*pos = atomic_load_explicit(		\
+				&aq->enqueue_pos.val,		\
+				memory_order_relaxed);		\
+		}						\
+	}							\
+	*buf = &e->buf;						\
+	return FI_SUCCESS;					\
+}								\
+static inline int name ## _rx_next(struct name *aq,		\
+		entrytype **buf, int64_t *pos)			\
+{								\
+	int64_t diff, seq;					\
+	struct name ## _entry *e;				\
+	*pos = atomic_load_explicit(&aq->dequeue_pos.val,	\
+			memory_order_relaxed);			\
+	for (;;) {						\
+		e = &aq->entry[*pos & aq->size_mask];		\
+		seq = e->seq.val;				\
+		diff = seq - (*pos + 1);			\
+		if (diff == 0) {				\
+			if(atomic_compare_exchange_weak(	\
+				&aq->dequeue_pos.val, pos,	\
+				*pos + 1))			\
+				break;				\
+		} else if (diff < 0) {				\
+			return -FI_ENOENT;			\
+		} else {					\
+			*pos = atomic_load_explicit(		\
+				&aq->dequeue_pos.val,		\
+				memory_order_relaxed);		\
+		}						\
+	}							\
+	*buf = &e->buf;						\
+	return FI_SUCCESS;					\
+}								\
+static inline void name ## _tx_advance(entrytype *buf,		\
+				int64_t pos)			\
+{								\
+	struct name ## _entry *e;				\
+	e = container_of(buf, struct name ## _entry, buf);	\
+	atomic_store_explicit(&e->seq.val, pos + 1,		\
+			      memory_order_release);		\
+}								\
+static inline void name ## _rx_advance (struct name *aq,	\
+			entrytype *buf,				\
+			int64_t pos)				\
+{								\
+	struct name ## _entry *e;				\
+	e = container_of(buf, struct name ## _entry, buf);	\
+	atomic_store_explicit(&e->seq.val,			\
+			      pos + aq->size_mask + 1,		\
+			      memory_order_release);		\
+}								\
+void dummy ## name (void) /* work-around global ; scope */
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* OFI_ATOMIC_QUEUE_H */
diff --git a/include/ofi_shm.h b/include/ofi_shm.h
index 9bbe9ffcf..ef440250e 100644
--- a/include/ofi_shm.h
+++ b/include/ofi_shm.h
@@ -45,6 +45,7 @@
 #include <ofi_rbuf.h>
 #include <ofi_tree.h>
 #include <ofi_hmem.h>
+#include <ofi_atomic_queue.h>
 
 #include <rdma/providers/fi_prov.h>
 
@@ -233,20 +234,20 @@ struct smr_region {
 	uint32_t	max_sar_buf_per_peer;
 	void		*base_addr;
 	pthread_spinlock_t	lock; /* lock for shm access
-				 Must hold smr->lock before tx/rx cq locks
-				 in order to progress or post recv */
+				 if both ep->tx_lock and this lock need to
+				 held, then ep->tx_lock needs to be held
+				 first */
 	ofi_atomic32_t	signal;
 
 	struct smr_map	*map;
 
 	size_t		total_size;
-	size_t		cmd_cnt; /* Doubles as a tracker for number of cmds AND
+	ofi_atomic64_t	cmd_cnt; /* Doubles as a tracker for number of cmds AND
 				    number of inject buffers available for use,
 				    to ensure 1:1 ratio of cmds to inject bufs.
 				    Might not always be paired consistently with
 				    cmd alloc/free depending on protocol
 				    (Ex. unexpected messages, RMA requests) */
-	size_t		sar_cnt;
 
 	/* offsets from start of smr_region */
 	size_t		cmd_queue_offset;
@@ -286,8 +287,21 @@ struct smr_sar_buf {
 	uint8_t		buf[SMR_SAR_SIZE];
 };
 
-OFI_DECLARE_CIRQUE(struct smr_cmd, smr_cmd_queue);
+/* TODO it is expected that a future patch will expand the smr_cmd
+ * structure to also include the rma information, thereby removing the
+ * need to have two commands in the cmd_entry. We can also remove the
+ * command entry completely and just use the smr_cmd
+ */
+struct smr_cmd_entry {
+	struct smr_cmd cmd;
+	struct smr_cmd rma_cmd;
+};
+
+/* Queue of offsets of the command blocks obtained from the command pool
+ * freestack 
+ */
 OFI_DECLARE_CIRQUE(struct smr_resp, smr_resp_queue);
+OFI_DECLARE_ATOMIQ(struct smr_cmd_entry, smr_cmd_queue);
 
 static inline struct smr_region *smr_peer_region(struct smr_region *smr, int i)
 {
diff --git a/prov/shm/src/smr.h b/prov/shm/src/smr.h
index 56fb826a9..8d8868348 100644
--- a/prov/shm/src/smr.h
+++ b/prov/shm/src/smr.h
@@ -147,6 +147,7 @@ struct smr_sar_entry {
 	size_t			iov_count;
 	enum fi_hmem_iface	iface;
 	uint64_t		device;
+	bool			in_use;
 };
 
 struct smr_cq {
@@ -366,7 +367,7 @@ typedef ssize_t (*smr_proto_func)(struct smr_ep *ep, struct smr_region *peer_smr
 		int64_t id, int64_t peer_id, uint32_t op, uint64_t tag,
 		uint64_t data, uint64_t op_flags, enum fi_hmem_iface iface,
 		uint64_t device, const struct iovec *iov, size_t iov_count,
-		size_t total_len, void *context);
+		size_t total_len, void *context, struct smr_cmd *cmd);
 extern smr_proto_func smr_proto_ops[smr_src_max];
 
 int smr_write_err_comp(struct util_cq *cq, void *context,
@@ -448,5 +449,68 @@ static inline int smr_cma_loop(pid_t pid, struct iovec *local,
 	}
 }
 
+static inline struct smr_cmd_entry *
+smr_get_cmd(struct smr_region *smr, int64_t *pos)
+{
+	struct smr_cmd_entry *e = NULL;
+	int ret;
+
+	ret = smr_cmd_queue_tx_next(smr_cmd_queue(smr), &e, pos);
+
+	if (ret == -FI_ENOENT)
+		return NULL;
+
+	return e;
+}
+
+static inline void smr_queue_cmd(struct smr_cmd_entry *e,
+				 int64_t pos)
+{
+	smr_cmd_queue_tx_advance(e, pos);
+}
+
+static inline struct smr_cmd_entry *
+smr_recv_cmd(struct smr_region *smr, int64_t *pos)
+{
+	struct smr_cmd_entry *e = NULL;
+	int ret;
+
+	ret = smr_cmd_queue_rx_next(smr_cmd_queue(smr), &e, pos);
+
+	if (ret == -FI_ENOENT)
+		return NULL;
+
+	return e;
+}
+
+static inline void
+smr_discard_cmd(struct smr_region *smr,
+		struct smr_cmd_entry *e,
+		int64_t pos)
+{
+	smr_cmd_queue_rx_advance(smr_cmd_queue(smr), e, pos);
+}
+
+static inline struct smr_inject_buf *
+smr_get_txbuf(struct smr_region *smr)
+{
+	struct smr_inject_buf * txbuf;
+
+	pthread_spin_lock(&smr->lock);
+	txbuf = smr_freestack_pop(smr_inject_pool(smr));
+	pthread_spin_unlock(&smr->lock);
+
+	return txbuf;
+}
+
+static inline void
+smr_discard_txbuf(struct smr_region *smr,
+		  struct smr_inject_buf *tx_buf)
+{
+	pthread_spin_lock(&smr->lock);
+	smr_freestack_push(smr_inject_pool(smr), tx_buf);
+	pthread_spin_unlock(&smr->lock);
+}
+
 int smr_unexp_start(struct fi_peer_rx_entry *rx_entry);
 #endif
diff --git a/prov/shm/src/smr_atomic.c b/prov/shm/src/smr_atomic.c
index a18864534..27b244a22 100644
--- a/prov/shm/src/smr_atomic.c
+++ b/prov/shm/src/smr_atomic.c
@@ -69,17 +69,11 @@ static void smr_do_atomic_inline(struct smr_ep *ep, struct smr_region *peer_smr,
 			uint64_t op_flags, enum fi_hmem_iface iface,
 			uint64_t device, uint8_t datatype, uint8_t atomic_op,
 			const struct iovec *iov, size_t iov_count,
-			size_t total_len)
+			size_t total_len, struct smr_cmd *cmd)
 {
-	struct smr_cmd *cmd;
-
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
 	smr_generic_format(cmd, peer_id, op, 0, 0, op_flags);
 	smr_generic_atomic_format(cmd, datatype, atomic_op);
 	smr_format_inline_atomic(cmd, iface, device, iov, iov_count);
-
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
 }
 
 static void smr_format_inject_atomic(struct smr_cmd *cmd,
@@ -130,15 +124,14 @@ static ssize_t smr_do_atomic_inject(struct smr_ep *ep, struct smr_region *peer_s
 			const struct iovec *iov, size_t iov_count,
 			const struct iovec *resultv, size_t result_count,
 			const struct iovec *compv, size_t comp_count,
-			size_t total_len, void *context, uint16_t smr_flags)
+			size_t total_len, void *context, uint16_t smr_flags,
+			struct smr_cmd *cmd)
 {
-	struct smr_cmd *cmd;
 	struct smr_inject_buf *tx_buf;
 	struct smr_tx_entry *pend;
 	struct smr_resp *resp;
 
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
-	tx_buf = smr_freestack_pop(smr_inject_pool(peer_smr));
+	tx_buf = smr_get_txbuf(peer_smr);
 
 	smr_generic_format(cmd, peer_id, op, 0, 0, op_flags);
 	smr_generic_atomic_format(cmd, datatype, atomic_op);
@@ -148,7 +141,7 @@ static ssize_t smr_do_atomic_inject(struct smr_ep *ep, struct smr_region *peer_s
 
 	if (smr_flags & SMR_RMA_REQ || op_flags & FI_DELIVERY_COMPLETE) {
 		if (ofi_cirque_isfull(smr_resp_queue(ep->region))) {
-			smr_freestack_push(smr_inject_pool(peer_smr), tx_buf);
+			smr_discard_txbuf(peer_smr, tx_buf);
 			return -FI_EAGAIN;
 		}
 		resp = ofi_cirque_next(smr_resp_queue(ep->region));
@@ -160,8 +153,6 @@ static ssize_t smr_do_atomic_inject(struct smr_ep *ep, struct smr_region *peer_s
 	}
 
 	cmd->msg.hdr.op_flags |= smr_flags;
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
 
 	return FI_SUCCESS;
 }
@@ -186,7 +177,7 @@ static ssize_t smr_generic_atomic(struct smr_ep *ep,
 			enum fi_op atomic_op, void *context, uint32_t op,
 			uint64_t op_flags)
 {
-	struct smr_cmd *cmd;
+	struct smr_cmd_entry *cmd;
 	struct smr_region *peer_smr;
 	struct iovec iov[SMR_IOV_LIMIT];
 	struct iovec compare_iov[SMR_IOV_LIMIT];
@@ -198,6 +189,7 @@ static ssize_t smr_generic_atomic(struct smr_ep *ep,
 	int proto;
 	ssize_t ret = 0;
 	size_t total_len;
+	int64_t pos;
 
 	assert(count <= SMR_IOV_LIMIT);
 	assert(result_count <= SMR_IOV_LIMIT);
@@ -211,11 +203,13 @@ static ssize_t smr_generic_atomic(struct smr_ep *ep,
 	peer_id = smr_peer_data(ep->region)[id].addr.id;
 	peer_smr = smr_peer_region(ep->region, id);
 
-	pthread_spin_lock(&peer_smr->lock);
-	if (peer_smr->cmd_cnt < 2 || smr_peer_data(ep->region)[id].sar_status) {
-		ret = -FI_EAGAIN;
-		goto unlock_region;
-	}
+	if (ofi_atomic_get64(&peer_smr->cmd_cnt) < 2 ||
+	    smr_peer_data(ep->region)[id].sar_status)
+		return -FI_EAGAIN;
+
+	cmd = smr_get_cmd(peer_smr, &pos);
+	if (!cmd)
+		return -FI_EAGAIN;
 
 	ofi_spin_lock(&ep->tx_lock);
 	total_len = ofi_datatype_size(datatype) * ofi_total_ioc_cnt(ioc, count);
@@ -252,15 +246,17 @@ static ssize_t smr_generic_atomic(struct smr_ep *ep,
 	if (proto == smr_src_inline) {
 		smr_do_atomic_inline(ep, peer_smr, id, peer_id, ofi_op_atomic,
 			 	     op_flags, iface, device, datatype, atomic_op,
-				     iov, count, total_len);
+				     iov, count, total_len, &cmd->cmd);
 	} else {
 		ret = smr_do_atomic_inject(ep, peer_smr, id, peer_id, op,
 				op_flags, iface, device, datatype, atomic_op,
 				iov, count, result_iov, result_count,
 				compare_iov, compare_count, total_len, context,
-				smr_flags);
-		if (ret)
+				smr_flags, &cmd->cmd);
+		if (ret) {
+			smr_discard_cmd(peer_smr, cmd, pos);
 			goto unlock_cq;
+		}
 	}
 
 	if (!(smr_flags & SMR_RMA_REQ) && !(op_flags & FI_DELIVERY_COMPLETE)) {
@@ -271,15 +267,11 @@ static ssize_t smr_generic_atomic(struct smr_ep *ep,
 		}
 	}
 
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
-	smr_format_rma_ioc(cmd, rma_ioc, rma_count);
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
+	smr_format_rma_ioc(&cmd->rma_cmd, rma_ioc, rma_count);
+	smr_queue_cmd(cmd, pos);
 	smr_signal(peer_smr);
 unlock_cq:
 	ofi_spin_unlock(&ep->tx_lock);
-unlock_region:
-	pthread_spin_unlock(&peer_smr->lock);
 	return ret;
 }
 
@@ -343,7 +335,7 @@ static ssize_t smr_atomic_inject(struct fid_ep *ep_fid, const void *buf,
 			size_t count, fi_addr_t dest_addr, uint64_t addr,
 			uint64_t key, enum fi_datatype datatype, enum fi_op op)
 {
-	struct smr_cmd *cmd;
+	struct smr_cmd_entry *cmd;
 	struct smr_ep *ep;
 	struct smr_region *peer_smr;
 	struct iovec iov;
@@ -351,6 +343,7 @@ static ssize_t smr_atomic_inject(struct fid_ep *ep_fid, const void *buf,
 	int64_t id, peer_id;
 	ssize_t ret = 0;
 	size_t total_len;
+	int64_t pos;
 
 	ep = container_of(ep_fid, struct smr_ep, util_ep.ep_fid.fid);
 
@@ -361,13 +354,16 @@ static ssize_t smr_atomic_inject(struct fid_ep *ep_fid, const void *buf,
 	peer_id = smr_peer_data(ep->region)[id].addr.id;
 	peer_smr = smr_peer_region(ep->region, id);
 
-	pthread_spin_lock(&peer_smr->lock);
-	if (peer_smr->cmd_cnt < 2 || smr_peer_data(ep->region)[id].sar_status) {
+	if (ofi_atomic_get64(&peer_smr->cmd_cnt) < 2 ||
+	    smr_peer_data(ep->region)[id].sar_status) {
 		ret = -FI_EAGAIN;
-		goto unlock_region;
+		goto out;
 	}
 
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
+	cmd = smr_get_cmd(peer_smr, &pos);
+	if (!cmd)
+		return -FI_EAGAIN;
+
 	total_len = count * ofi_datatype_size(datatype);
 	assert(total_len <= SMR_INJECT_SIZE);
 
@@ -381,25 +377,24 @@ static ssize_t smr_atomic_inject(struct fid_ep *ep_fid, const void *buf,
 	if (total_len <= SMR_MSG_DATA_LEN) {
 		smr_do_atomic_inline(ep, peer_smr, id, peer_id, ofi_op_atomic,
 			 	     0, FI_HMEM_SYSTEM, 0, datatype, op,
-				     &iov, 1, total_len);
+				     &iov, 1, total_len, &cmd->cmd);
 	} else if (total_len <= SMR_INJECT_SIZE) {
 		ret = smr_do_atomic_inject(ep, peer_smr, id, peer_id,
 				ofi_op_atomic, 0, FI_HMEM_SYSTEM, 0, datatype,
 				op, &iov, 1, NULL, 0, NULL, 0, total_len,
-				NULL, 0);
-		if (ret)
-			goto unlock_region;
+				NULL, 0, &cmd->cmd);
+		if (ret) {
+			smr_discard_cmd(peer_smr, cmd, pos);
+			goto out;
+		}
 	}
 
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
-	smr_format_rma_ioc(cmd, &rma_ioc, 1);
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
+	smr_format_rma_ioc(&cmd->rma_cmd, &rma_ioc, 1);
+	smr_queue_cmd(cmd, pos);
 	smr_signal(peer_smr);
 
 	ofi_ep_tx_cntr_inc_func(&ep->util_ep, ofi_op_atomic);
-unlock_region:
-	pthread_spin_unlock(&peer_smr->lock);
+out:
 	return ret;
 }
 
diff --git a/prov/shm/src/smr_ep.c b/prov/shm/src/smr_ep.c
index e2c666a83..96819f532 100644
--- a/prov/shm/src/smr_ep.c
+++ b/prov/shm/src/smr_ep.c
@@ -204,35 +204,31 @@ static struct fi_ops_ep smr_ep_ops = {
 static void smr_send_name(struct smr_ep *ep, int64_t id)
 {
 	struct smr_region *peer_smr;
-	struct smr_cmd *cmd;
+	struct smr_cmd_entry *cmd;
 	struct smr_inject_buf *tx_buf;
+	int64_t pos;
 
 	peer_smr = smr_peer_region(ep->region, id);
 
-	pthread_spin_lock(&peer_smr->lock);
-
-	if (smr_peer_data(ep->region)[id].name_sent || !peer_smr->cmd_cnt)
-		goto out;
+	if (smr_peer_data(ep->region)[id].name_sent ||
+	    !ofi_atomic_get64(&peer_smr->cmd_cnt))
+		return;
 
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
+	cmd = smr_get_cmd(peer_smr, &pos);
+	tx_buf = smr_get_txbuf(peer_smr);
 
-	cmd->msg.hdr.op = SMR_OP_MAX + ofi_ctrl_connreq;
-	cmd->msg.hdr.id = id;
-	cmd->msg.hdr.data = ep->region->pid;
+	cmd->cmd.msg.hdr.op = SMR_OP_MAX + ofi_ctrl_connreq;
+	cmd->cmd.msg.hdr.id = id;
+	cmd->cmd.msg.hdr.data = ep->region->pid;
 
-	tx_buf = smr_freestack_pop(smr_inject_pool(peer_smr));
-	cmd->msg.hdr.src_data = smr_get_offset(peer_smr, tx_buf);
+	cmd->cmd.msg.hdr.src_data = smr_get_offset(peer_smr, tx_buf);
 
-	cmd->msg.hdr.size = strlen(ep->name) + 1;
-	memcpy(tx_buf->data, ep->name, cmd->msg.hdr.size);
+	cmd->cmd.msg.hdr.size = strlen(ep->name) + 1;
+	memcpy(tx_buf->data, ep->name, cmd->cmd.msg.hdr.size);
 
 	smr_peer_data(ep->region)[id].name_sent = 1;
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
+	smr_queue_cmd(cmd, pos);
 	smr_signal(peer_smr);
-
-out:
-	pthread_spin_unlock(&peer_smr->lock);
 }
 
 int64_t smr_verify_peer(struct smr_ep *ep, fi_addr_t fi_addr)
@@ -559,27 +555,36 @@ static int smr_format_sar(struct smr_ep *ep, struct smr_cmd *cmd,
 	int i, ret;
 	uint32_t sar_needed;
 
-	if (!peer_smr->sar_cnt)
+	if (peer_smr->max_sar_buf_per_peer == 0)
 		return -FI_EAGAIN;
 
-	if (peer_smr->max_sar_buf_per_peer == 0)
+	ofi_ep_lock_acquire(&ep->util_ep);
+	if (smr_peer_data(ep->region)[id].sar_status) {
+		ofi_ep_lock_release(&ep->util_ep);
 		return -FI_EAGAIN;
+	}
+	smr_peer_data(smr)[id].sar_status = SMR_STATUS_SAR_READY;
+	ofi_ep_lock_release(&ep->util_ep);
 
 	sar_needed = (total_len + SMR_SAR_SIZE - 1) / SMR_SAR_SIZE;
 	cmd->msg.data.buf_batch_size = MIN(SMR_BUF_BATCH_MAX,
 			MIN(peer_smr->max_sar_buf_per_peer, sar_needed));
 
+	pthread_spin_lock(&peer_smr->lock);
 	for (i = 0; i < cmd->msg.data.buf_batch_size; i++) {
 		if (smr_freestack_isempty(smr_sar_pool(peer_smr))) {
 			cmd->msg.data.buf_batch_size = i;
-			if (i == 0)
+			if (i == 0) {
+				pthread_spin_unlock(&peer_smr->lock);
 				return -FI_EAGAIN;
+			}
 			break;
 		}
 
 		cmd->msg.data.sar[i] =
 			smr_freestack_pop_by_index(smr_sar_pool(peer_smr));
 	}
+	pthread_spin_unlock(&peer_smr->lock);
 
 	resp->status = SMR_STATUS_SAR_FREE;
 	cmd->msg.hdr.op_src = smr_src_sar;
@@ -608,10 +613,6 @@ static int smr_format_sar(struct smr_ep *ep, struct smr_cmd *cmd,
 					&pending->bytes_done, &pending->next);
 		}
 	}
-
-	peer_smr->sar_cnt--;
-	smr_peer_data(smr)[id].sar_status = SMR_STATUS_SAR_READY;
-
 	return 0;
 }
 
@@ -658,17 +659,11 @@ static ssize_t smr_do_inline(struct smr_ep *ep, struct smr_region *peer_smr, int
 			     int64_t peer_id, uint32_t op, uint64_t tag, uint64_t data,
 			     uint64_t op_flags, enum fi_hmem_iface iface, uint64_t device,
 			     const struct iovec *iov, size_t iov_count, size_t total_len,
-			     void *context)
+			     void *context, struct smr_cmd *cmd)
 {
-	struct smr_cmd *cmd;
-
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
 	smr_generic_format(cmd, peer_id, op, tag, data, op_flags);
 	smr_format_inline(cmd, iface, device, iov, iov_count);
 
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
-
 	return FI_SUCCESS;
 }
 
@@ -676,20 +671,15 @@ static ssize_t smr_do_inject(struct smr_ep *ep, struct smr_region *peer_smr, int
 			     int64_t peer_id, uint32_t op, uint64_t tag, uint64_t data,
 			     uint64_t op_flags, enum fi_hmem_iface iface, uint64_t device,
 			     const struct iovec *iov, size_t iov_count, size_t total_len,
-			     void *context)
+			     void *context, struct smr_cmd *cmd)
 {
-	struct smr_cmd *cmd;
 	struct smr_inject_buf *tx_buf;
 
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
-	tx_buf = smr_freestack_pop(smr_inject_pool(peer_smr));
+	tx_buf = smr_get_txbuf(peer_smr);
 
 	smr_generic_format(cmd, peer_id, op, tag, data, op_flags);
 	smr_format_inject(cmd, iface, device, iov, iov_count, peer_smr, tx_buf);
 
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
-
 	return FI_SUCCESS;
 }
 
@@ -697,16 +687,14 @@ static ssize_t smr_do_iov(struct smr_ep *ep, struct smr_region *peer_smr, int64_
 			  int64_t peer_id, uint32_t op, uint64_t tag, uint64_t data,
 			  uint64_t op_flags, enum fi_hmem_iface iface, uint64_t device,
 		          const struct iovec *iov, size_t iov_count, size_t total_len,
-		          void *context)
+		          void *context, struct smr_cmd *cmd)
 {
-	struct smr_cmd *cmd;
 	struct smr_resp *resp;
 	struct smr_tx_entry *pend;
 
 	if (ofi_cirque_isfull(smr_resp_queue(ep->region)))
 		return -FI_EAGAIN;
 
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
 	resp = ofi_cirque_next(smr_resp_queue(ep->region));
 	pend = ofi_freestack_pop(ep->pend_fs);
 
@@ -716,9 +704,6 @@ static ssize_t smr_do_iov(struct smr_ep *ep, struct smr_region *peer_smr, int64_
 			     iov_count, op_flags, id, resp);
 	ofi_cirque_commit(smr_resp_queue(ep->region));
 
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
-
 	return FI_SUCCESS;
 }
 
@@ -726,9 +711,8 @@ static ssize_t smr_do_sar(struct smr_ep *ep, struct smr_region *peer_smr, int64_
 			  int64_t peer_id, uint32_t op, uint64_t tag, uint64_t data,
 			  uint64_t op_flags, enum fi_hmem_iface iface, uint64_t device,
 		          const struct iovec *iov, size_t iov_count, size_t total_len,
-		          void *context)
+		          void *context, struct smr_cmd *cmd)
 {
-	struct smr_cmd *cmd;
 	struct smr_resp *resp;
 	struct smr_tx_entry *pend;
 	int ret;
@@ -736,7 +720,6 @@ static ssize_t smr_do_sar(struct smr_ep *ep, struct smr_region *peer_smr, int64_
 	if (ofi_cirque_isfull(smr_resp_queue(ep->region)))
 		return -FI_EAGAIN;
 
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
 	resp = ofi_cirque_next(smr_resp_queue(ep->region));
 	pend = ofi_freestack_pop(ep->pend_fs);
 
@@ -752,9 +735,6 @@ static ssize_t smr_do_sar(struct smr_ep *ep, struct smr_region *peer_smr, int64_
 			     iov_count, op_flags, id, resp);
 	ofi_cirque_commit(smr_resp_queue(ep->region));
 
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
-
 	return FI_SUCCESS;
 }
 
@@ -762,9 +742,8 @@ static ssize_t smr_do_ipc(struct smr_ep *ep, struct smr_region *peer_smr, int64_
 			  int64_t peer_id, uint32_t op, uint64_t tag, uint64_t data,
 			  uint64_t op_flags, enum fi_hmem_iface iface, uint64_t device,
 		          const struct iovec *iov, size_t iov_count, size_t total_len,
-		          void *context)
+		          void *context, struct smr_cmd *cmd)
 {
-	struct smr_cmd *cmd;
 	struct smr_resp *resp;
 	struct smr_tx_entry *pend;
 	int ret = -FI_EAGAIN;
@@ -772,7 +751,6 @@ static ssize_t smr_do_ipc(struct smr_ep *ep, struct smr_region *peer_smr, int64_
 	if (ofi_cirque_isfull(smr_resp_queue(ep->region)))
 		return -FI_EAGAIN;
 
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
 	resp = ofi_cirque_next(smr_resp_queue(ep->region));
 	pend = ofi_freestack_pop(ep->pend_fs);
 
@@ -792,16 +770,13 @@ static ssize_t smr_do_ipc(struct smr_ep *ep, struct smr_region *peer_smr, int64_
 		ofi_freestack_push(ep->pend_fs, pend);
 		return smr_do_sar(ep, peer_smr, id, peer_id, op, tag, data,
 				  op_flags, iface, device, iov, iov_count,
-				  total_len, context);
+				  total_len, context, cmd);
 	}
 
 	smr_format_pend_resp(pend, cmd, context, iface, device, iov,
 			     iov_count, op_flags, id, resp);
 	ofi_cirque_commit(smr_resp_queue(ep->region));
 
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
-
 	return FI_SUCCESS;
 }
 
@@ -809,9 +784,8 @@ static ssize_t smr_do_mmap(struct smr_ep *ep, struct smr_region *peer_smr, int64
 			   int64_t peer_id, uint32_t op, uint64_t tag, uint64_t data,
 			   uint64_t op_flags, enum fi_hmem_iface iface, uint64_t device,
 		           const struct iovec *iov, size_t iov_count, size_t total_len,
-		           void *context)
+		           void *context, struct smr_cmd *cmd)
 {
-	struct smr_cmd *cmd;
 	struct smr_resp *resp;
 	struct smr_tx_entry *pend;
 	int ret;
@@ -819,7 +793,6 @@ static ssize_t smr_do_mmap(struct smr_ep *ep, struct smr_region *peer_smr, int64
 	if (ofi_cirque_isfull(smr_resp_queue(ep->region)))
 		return -FI_EAGAIN;
 
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
 	resp = ofi_cirque_next(smr_resp_queue(ep->region));
 	pend = ofi_freestack_pop(ep->pend_fs);
 
@@ -834,9 +807,6 @@ static ssize_t smr_do_mmap(struct smr_ep *ep, struct smr_region *peer_smr, int64
 			     iov_count, op_flags, id, resp);
 	ofi_cirque_commit(smr_resp_queue(ep->region));
 
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
-
 	return FI_SUCCESS;
 }
 
diff --git a/prov/shm/src/smr_msg.c b/prov/shm/src/smr_msg.c
index d11366a1c..998f88c05 100644
--- a/prov/shm/src/smr_msg.c
+++ b/prov/shm/src/smr_msg.c
@@ -295,6 +295,8 @@ static ssize_t smr_generic_sendmsg(struct smr_ep *ep, const struct iovec *iov,
 	size_t total_len;
 	bool use_ipc;
 	int proto;
+	struct smr_cmd_entry *cmd;
+	int64_t pos;
 
 	assert(iov_count <= SMR_IOV_LIMIT);
 
@@ -305,11 +307,13 @@ static ssize_t smr_generic_sendmsg(struct smr_ep *ep, const struct iovec *iov,
 	peer_id = smr_peer_data(ep->region)[id].addr.id;
 	peer_smr = smr_peer_region(ep->region, id);
 
-	pthread_spin_lock(&peer_smr->lock);
-	if (!peer_smr->cmd_cnt || smr_peer_data(ep->region)[id].sar_status) {
-		ret = -FI_EAGAIN;
-		goto unlock_region;
-	}
+	if (!ofi_atomic_get64(&peer_smr->cmd_cnt) ||
+	    smr_peer_data(ep->region)[id].sar_status)
+		return -FI_EAGAIN;
+
+	cmd = smr_get_cmd(peer_smr, &pos);
+	if (!cmd)
+		return -FI_EAGAIN;
 
 	ofi_spin_lock(&ep->tx_lock);
 	iface = smr_get_mr_hmem_iface(ep->util_ep.domain, desc, &device);
@@ -327,10 +331,13 @@ static ssize_t smr_generic_sendmsg(struct smr_ep *ep, const struct iovec *iov,
 				 op, total_len, op_flags);
 
 	ret = smr_proto_ops[proto](ep, peer_smr, id, peer_id, op, tag, data, op_flags,
-				   iface, device, iov, iov_count, total_len, context);
-	if (ret)
+				   iface, device, iov, iov_count, total_len,
+				   context, &cmd->cmd);
+	if (ret) {
+		smr_discard_cmd(peer_smr, cmd, pos);
 		goto unlock_cq;
-
+	}
+	smr_queue_cmd(cmd, pos);
 	smr_signal(peer_smr);
 
 	if (proto != smr_src_inline && proto != smr_src_inject)
@@ -345,8 +352,6 @@ static ssize_t smr_generic_sendmsg(struct smr_ep *ep, const struct iovec *iov,
 
 unlock_cq:
 	ofi_spin_unlock(&ep->tx_lock);
-unlock_region:
-	pthread_spin_unlock(&peer_smr->lock);
 	return ret;
 }
 
@@ -398,6 +403,8 @@ static ssize_t smr_generic_inject(struct fid_ep *ep_fid, const void *buf,
 	ssize_t ret = 0;
 	struct iovec msg_iov;
 	int proto;
+	struct smr_cmd_entry *cmd;
+	int64_t pos;
 
 	assert(len <= SMR_INJECT_SIZE);
 
@@ -413,22 +420,23 @@ static ssize_t smr_generic_inject(struct fid_ep *ep_fid, const void *buf,
 	peer_id = smr_peer_data(ep->region)[id].addr.id;
 	peer_smr = smr_peer_region(ep->region, id);
 
-	pthread_spin_lock(&peer_smr->lock);
-	if (!peer_smr->cmd_cnt || smr_peer_data(ep->region)[id].sar_status) {
-		ret = -FI_EAGAIN;
-		goto unlock;
-	}
+	if (!ofi_atomic_get64(&peer_smr->cmd_cnt) ||
+	    smr_peer_data(ep->region)[id].sar_status)
+		return -FI_EAGAIN;
 
+	cmd = smr_get_cmd(peer_smr, &pos);
+	if (!cmd)
+		return -FI_EAGAIN;
 	proto = len <= SMR_MSG_DATA_LEN ? smr_src_inline : smr_src_inject;
 	ret = smr_proto_ops[proto](ep, peer_smr, id, peer_id, op, tag, data,
-			op_flags, FI_HMEM_SYSTEM, 0, &msg_iov, 1, len, NULL);
+			op_flags, FI_HMEM_SYSTEM, 0, &msg_iov, 1, len,
+			NULL, &cmd->cmd);
+	smr_queue_cmd(cmd, pos);
 
 	assert(!ret);
 	ofi_ep_tx_cntr_inc_func(&ep->util_ep, op);
 
 	smr_signal(peer_smr);
-unlock:
-	pthread_spin_unlock(&peer_smr->lock);
 
 	return ret;
 }
@@ -703,4 +711,4 @@ struct fi_ops_tagged smr_srx_tag_ops = {
 	.inject = smr_tinject,
 	.senddata = smr_tsenddata,
 	.injectdata = smr_tinjectdata,
-};
\ No newline at end of file
+};
diff --git a/prov/shm/src/smr_progress.c b/prov/shm/src/smr_progress.c
index 32fa115bb..6a26b02d0 100644
--- a/prov/shm/src/smr_progress.c
+++ b/prov/shm/src/smr_progress.c
@@ -126,8 +126,9 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 					pending->iov_count, &pending->bytes_done,
 					&pending->next, pending);
 		if (pending->bytes_done != pending->cmd.msg.hdr.size ||
-		    resp->status != SMR_STATUS_SAR_FREE)
+		    resp->status != SMR_STATUS_SAR_FREE) {
 			return -FI_EAGAIN;
+		}
 
 		resp->status = SMR_STATUS_SUCCESS;
 		break;
@@ -194,30 +195,19 @@ static int smr_progress_resp_entry(struct smr_ep *ep, struct smr_resp *resp,
 			"unidentified operation type\n");
 	}
 
-	//Skip locking on transfers from self since we already have
-	//the ep->region->lock
-	if (peer_smr != ep->region) {
-		if (pthread_spin_trylock(&peer_smr->lock)) {
-			smr_signal(ep->region);
-			return -FI_EAGAIN;
-		}
-	}
-
-	peer_smr->cmd_cnt++;
+	ofi_atomic_inc64(&peer_smr->cmd_cnt);
 	if (tx_buf) {
-		smr_freestack_push(smr_inject_pool(peer_smr), tx_buf);
+		smr_discard_txbuf(peer_smr, tx_buf);
 	} else if (sar_buf) {
+		pthread_spin_lock(&peer_smr->lock);
 		for (i = pending->cmd.msg.data.buf_batch_size - 1; i >= 0; i--) {
 			smr_freestack_push_by_index(smr_sar_pool(peer_smr),
 					pending->cmd.msg.data.sar[i]);
 		}
-		peer_smr->sar_cnt++;
+		pthread_spin_unlock(&peer_smr->lock);
 		smr_peer_data(ep->region)[pending->peer_id].sar_status = 0;
 	}
 
-	if (peer_smr != ep->region)
-		pthread_spin_unlock(&peer_smr->lock);
-
 	return FI_SUCCESS;
 }
 
@@ -227,7 +217,6 @@ static void smr_progress_resp(struct smr_ep *ep)
 	struct smr_tx_entry *pending;
 	int ret;
 
-	pthread_spin_lock(&ep->region->lock);
 	ofi_spin_lock(&ep->tx_lock);
 	while (!ofi_cirque_isempty(smr_resp_queue(ep->region))) {
 		resp = ofi_cirque_head(smr_resp_queue(ep->region));
@@ -255,7 +244,6 @@ static void smr_progress_resp(struct smr_ep *ep)
 		ofi_cirque_discard(smr_resp_queue(ep->region));
 	}
 	ofi_spin_unlock(&ep->tx_lock);
-	pthread_spin_unlock(&ep->region->lock);
 }
 
 static int smr_progress_inline(struct smr_cmd *cmd, enum fi_hmem_iface iface,
@@ -295,7 +283,7 @@ static int smr_progress_inject(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 	tx_buf = smr_get_ptr(ep->region, inj_offset);
 
 	if (err) {
-		smr_freestack_push(smr_inject_pool(ep->region), tx_buf);
+		smr_discard_txbuf(ep->region, tx_buf);
 		return err;
 	}
 
@@ -308,7 +296,7 @@ static int smr_progress_inject(struct smr_cmd *cmd, enum fi_hmem_iface iface,
 		hmem_copy_ret = ofi_copy_to_hmem_iov(iface, device, iov,
 						     iov_count, 0, tx_buf->data,
 						     cmd->msg.hdr.size);
-		smr_freestack_push(smr_inject_pool(ep->region), tx_buf);
+		smr_discard_txbuf(ep->region, tx_buf);
 	}
 
 	if (hmem_copy_ret < 0) {
@@ -459,7 +447,11 @@ static struct smr_sar_entry *smr_progress_sar(struct smr_cmd *cmd,
 	memcpy(sar_iov, iov, sizeof(*iov) * iov_count);
 	(void) ofi_truncate_iov(sar_iov, &iov_count, cmd->msg.hdr.size);
 
+	ofi_ep_lock_acquire(&ep->util_ep);
 	sar_entry = ofi_freestack_pop(ep->sar_fs);
+	sar_entry->in_use = true;
+	dlist_insert_tail(&sar_entry->entry, &ep->sar_list);
+	ofi_ep_lock_release(&ep->util_ep);
 
 	if (cmd->msg.hdr.op == ofi_op_read_req)
 		smr_try_progress_to_sar(ep, peer_smr, smr_sar_pool(ep->region),
@@ -470,12 +462,16 @@ static struct smr_sar_entry *smr_progress_sar(struct smr_cmd *cmd,
 				smr_sar_pool(ep->region), resp, cmd, iface,
 				device, sar_iov, iov_count, total_len, &next,
 				sar_entry);
+	ofi_ep_lock_acquire(&ep->util_ep);
+	sar_entry->in_use = false;
 
 	if (*total_len == cmd->msg.hdr.size) {
+		dlist_remove(&sar_entry->entry);
 		ofi_freestack_push(ep->sar_fs, sar_entry);
+		ofi_ep_lock_release(&ep->util_ep);
 		return NULL;
 	}
-
+	ofi_ep_lock_release(&ep->util_ep);
 	sar_entry->cmd = *cmd;
 	sar_entry->bytes_done = *total_len;
 	sar_entry->next = next;
@@ -484,7 +480,6 @@ static struct smr_sar_entry *smr_progress_sar(struct smr_cmd *cmd,
 	sar_entry->rx_entry = rx_entry ? rx_entry : NULL;
 	sar_entry->iface = iface;
 	sar_entry->device = device;
-	dlist_insert_tail(&sar_entry->entry, &ep->sar_list);
 	*total_len = cmd->msg.hdr.size;
 	return sar_entry;
 }
@@ -654,7 +649,7 @@ static int smr_progress_inject_atomic(struct smr_cmd *cmd, struct fi_ioc *ioc,
 
 out:
 	if (!(cmd->msg.hdr.op_flags & SMR_RMA_REQ))
-		smr_freestack_push(smr_inject_pool(ep->region), tx_buf);
+		smr_discard_txbuf(ep->region, tx_buf);
 
 	return err;
 }
@@ -678,13 +673,13 @@ static int smr_start_common(struct smr_ep *ep, struct smr_cmd *cmd,
 		err = smr_progress_inline(cmd, iface, device,
 					  rx_entry->iov, rx_entry->count,
 					  &total_len);
-		ep->region->cmd_cnt++;
+		ofi_atomic_inc64(&ep->region->cmd_cnt);
 		break;
 	case smr_src_inject:
 		err = smr_progress_inject(cmd, iface, device,
 					  rx_entry->iov, rx_entry->count,
 					  &total_len, ep, 0);
-		ep->region->cmd_cnt++;
+		ofi_atomic_inc64(&ep->region->cmd_cnt);
 		break;
 	case smr_src_iov:
 		err = smr_progress_iov(cmd, rx_entry->iov, rx_entry->count,
@@ -743,10 +738,8 @@ int smr_unexp_start(struct fi_peer_rx_entry *rx_entry)
 	struct smr_cmd_ctx *cmd_ctx = rx_entry->peer_context;
 	int ret;
 
-	pthread_spin_lock(&cmd_ctx->ep->region->lock);
 	ret = smr_start_common(cmd_ctx->ep, &cmd_ctx->cmd, rx_entry);
 	ofi_freestack_push(cmd_ctx->ep->cmd_ctx_fs, cmd_ctx);
-	pthread_spin_unlock(&cmd_ctx->ep->region->lock);
 
 	return ret;
 }
@@ -780,9 +773,7 @@ static void smr_progress_connreq(struct smr_ep *ep, struct smr_cmd *cmd)
 	smr_peer_data(peer_smr)[cmd->msg.hdr.id].addr.id = idx;
 	smr_peer_data(ep->region)[idx].addr.id = cmd->msg.hdr.id;
 
-	smr_freestack_push(smr_inject_pool(ep->region), tx_buf);
-	ofi_cirque_discard(smr_cmd_queue(ep->region));
-	ep->region->cmd_cnt++;
+	smr_discard_txbuf(ep->region, tx_buf);
 	assert(ep->region->map->num_peers > 0);
 	ep->region->max_sar_buf_per_peer = SMR_MAX_PEERS /
 		ep->region->map->num_peers;
@@ -843,15 +834,14 @@ static int smr_progress_cmd_msg(struct smr_ep *ep, struct smr_cmd *cmd)
 	ret = smr_start_common(ep, cmd, rx_entry);
 
 out:
-	ofi_cirque_discard(smr_cmd_queue(ep->region));
 	return ret < 0 ? ret : 0;
 }
 
-static int smr_progress_cmd_rma(struct smr_ep *ep, struct smr_cmd *cmd)
+static int smr_progress_cmd_rma(struct smr_ep *ep, struct smr_cmd *cmd,
+				struct smr_cmd *rma_cmd)
 {
 	struct smr_region *peer_smr;
 	struct smr_domain *domain;
-	struct smr_cmd *rma_cmd;
 	struct smr_resp *resp;
 	struct iovec iov[SMR_IOV_LIMIT];
 	size_t iov_count;
@@ -864,10 +854,6 @@ static int smr_progress_cmd_rma(struct smr_ep *ep, struct smr_cmd *cmd)
 	domain = container_of(ep->util_ep.domain, struct smr_domain,
 			      util_domain);
 
-	ofi_cirque_discard(smr_cmd_queue(ep->region));
-	ep->region->cmd_cnt++;
-	rma_cmd = ofi_cirque_head(smr_cmd_queue(ep->region));
-
 	ofi_genlock_lock(&domain->util_domain.lock);
 	for (iov_count = 0; iov_count < rma_cmd->rma.rma_count; iov_count++) {
 		ret = ofi_mr_map_verify(&domain->util_domain.mr_map,
@@ -890,17 +876,13 @@ static int smr_progress_cmd_rma(struct smr_ep *ep, struct smr_cmd *cmd)
 	}
 	ofi_genlock_unlock(&domain->util_domain.lock);
 
-	ofi_cirque_discard(smr_cmd_queue(ep->region));
-	if (ret) {
-		ep->region->cmd_cnt++;
-		return ret;
-	}
+	if (ret)
+		goto out;
 
 	switch (cmd->msg.hdr.op_src) {
 	case smr_src_inline:
 		err = smr_progress_inline(cmd, iface, device, iov, iov_count,
 					  &total_len);
-		ep->region->cmd_cnt++;
 		break;
 	case smr_src_inject:
 		err = smr_progress_inject(cmd, iface, device, iov, iov_count,
@@ -910,8 +892,6 @@ static int smr_progress_cmd_rma(struct smr_ep *ep, struct smr_cmd *cmd)
 			resp = smr_get_ptr(peer_smr, cmd->msg.hdr.data);
 			resp->status = -err;
 			smr_signal(peer_smr);
-		} else {
-			ep->region->cmd_cnt++;
 		}
 		break;
 	case smr_src_iov:
@@ -924,7 +904,7 @@ static int smr_progress_cmd_rma(struct smr_ep *ep, struct smr_cmd *cmd)
 	case smr_src_sar:
 		if (smr_progress_sar(cmd, NULL, iface, device, iov, iov_count,
 				     &total_len, ep))
-			return ret;
+			goto out;
 		break;
 	case smr_src_ipc:
 		err = smr_progress_ipc(cmd, iface, device, iov, iov_count,
@@ -954,27 +934,24 @@ static int smr_progress_cmd_rma(struct smr_ep *ep, struct smr_cmd *cmd)
 		"unable to process rx completion\n");
 	}
 
+out:
 	return ret;
 }
 
-static int smr_progress_cmd_atomic(struct smr_ep *ep, struct smr_cmd *cmd)
+static int smr_progress_cmd_atomic(struct smr_ep *ep, struct smr_cmd *cmd,
+				   struct smr_cmd *rma_cmd)
 {
 	struct smr_region *peer_smr;
 	struct smr_domain *domain;
-	struct smr_cmd *rma_cmd;
 	struct smr_resp *resp;
 	struct fi_ioc ioc[SMR_IOV_LIMIT];
 	size_t ioc_count;
 	size_t total_len = 0;
-	int err, ret = 0;
+	int err = 0, ret = 0;
 
 	domain = container_of(ep->util_ep.domain, struct smr_domain,
 			      util_domain);
 
-	ofi_cirque_discard(smr_cmd_queue(ep->region));
-	ep->region->cmd_cnt++;
-	rma_cmd = ofi_cirque_head(smr_cmd_queue(ep->region));
-
 	for (ioc_count = 0; ioc_count < rma_cmd->rma.rma_count; ioc_count++) {
 		ret = ofi_mr_verify(&domain->util_domain.mr_map,
 				rma_cmd->rma.rma_ioc[ioc_count].count *
@@ -989,11 +966,8 @@ static int smr_progress_cmd_atomic(struct smr_ep *ep, struct smr_cmd *cmd)
 		ioc[ioc_count].addr = (void *) rma_cmd->rma.rma_ioc[ioc_count].addr;
 		ioc[ioc_count].count = rma_cmd->rma.rma_ioc[ioc_count].count;
 	}
-	ofi_cirque_discard(smr_cmd_queue(ep->region));
-	if (ret) {
-		ep->region->cmd_cnt++;
-		return ret;
-	}
+	if (ret)
+		goto out;
 
 	switch (cmd->msg.hdr.op_src) {
 	case smr_src_inline:
@@ -1012,8 +986,6 @@ static int smr_progress_cmd_atomic(struct smr_ep *ep, struct smr_cmd *cmd)
 		resp = smr_get_ptr(peer_smr, cmd->msg.hdr.data);
 		resp->status = -err;
 		smr_signal(peer_smr);
-	} else {
-		ep->region->cmd_cnt++;
 	}
 
 	if (err) {
@@ -1032,50 +1004,54 @@ static int smr_progress_cmd_atomic(struct smr_ep *ep, struct smr_cmd *cmd)
 	if (ret) {
 		FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 			"unable to process rx completion\n");
-		return ret;
+		err = ret;
 	}
 
+out:
 	return err;
 }
 
 static void smr_progress_cmd(struct smr_ep *ep)
 {
-	struct smr_cmd *cmd;
+	struct smr_cmd_entry *ce;
 	int ret = 0;
+	int64_t pos;
 
-	pthread_spin_lock(&ep->region->lock);
-	while (!ofi_cirque_isempty(smr_cmd_queue(ep->region))) {
-		cmd = ofi_cirque_head(smr_cmd_queue(ep->region));
-
-		switch (cmd->msg.hdr.op) {
+	while ((ce = smr_recv_cmd(ep->region, &pos))) {
+		switch (ce->cmd.msg.hdr.op) {
 		case ofi_op_msg:
 		case ofi_op_tagged:
-			ret = smr_progress_cmd_msg(ep, cmd);
+			ret = smr_progress_cmd_msg(ep, &ce->cmd);
 			break;
 		case ofi_op_write:
 		case ofi_op_read_req:
-			ret = smr_progress_cmd_rma(ep, cmd);
+			ret = smr_progress_cmd_rma(ep, &ce->cmd,
+				&ce->rma_cmd);
+			ofi_atomic_inc64(&ep->region->cmd_cnt);
 			break;
 		case ofi_op_write_async:
 		case ofi_op_read_async:
 			ofi_ep_rx_cntr_inc_func(&ep->util_ep,
-						cmd->msg.hdr.op);
-			ofi_cirque_discard(smr_cmd_queue(ep->region));
-			ep->region->cmd_cnt++;
+						ce->cmd.msg.hdr.op);
+			ofi_atomic_inc64(&ep->region->cmd_cnt);
 			break;
 		case ofi_op_atomic:
 		case ofi_op_atomic_fetch:
 		case ofi_op_atomic_compare:
-			ret = smr_progress_cmd_atomic(ep, cmd);
+			ret = smr_progress_cmd_atomic(ep, &ce->cmd,
+				&ce->rma_cmd);
+			ofi_atomic_inc64(&ep->region->cmd_cnt);
 			break;
 		case SMR_OP_MAX + ofi_ctrl_connreq:
-			smr_progress_connreq(ep, cmd);
+			smr_progress_connreq(ep, &ce->cmd);
+			ofi_atomic_inc64(&ep->region->cmd_cnt);
 			break;
 		default:
 			FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 				"unidentified operation type\n");
 			ret = -FI_EINVAL;
 		}
+		smr_discard_cmd(ep->region, ce, pos);
 		if (ret) {
 			smr_signal(ep->region);
 			if (ret != -FI_EAGAIN) {
@@ -1085,7 +1061,6 @@ static void smr_progress_cmd(struct smr_ep *ep)
 			break;
 		}
 	}
-	pthread_spin_unlock(&ep->region->lock);
 }
 
 static void smr_progress_sar_list(struct smr_ep *ep)
@@ -1098,9 +1073,13 @@ static void smr_progress_sar_list(struct smr_ep *ep)
 	uint64_t comp_flags;
 	int ret;
 
-	pthread_spin_lock(&ep->region->lock);
+	ofi_ep_lock_acquire(&ep->util_ep);
 	dlist_foreach_container_safe(&ep->sar_list, struct smr_sar_entry,
 				     sar_entry, entry, tmp) {
+		if (sar_entry->in_use)
+			continue;
+		sar_entry->in_use = true;
+		ofi_ep_lock_release(&ep->util_ep);
 		peer_smr = smr_peer_region(ep->region, sar_entry->cmd.msg.hdr.id);
 		resp = smr_get_ptr(peer_smr, sar_entry->cmd.msg.hdr.src_data);
 		if (sar_entry->cmd.msg.hdr.op == ofi_op_read_req)
@@ -1138,13 +1117,18 @@ static void smr_progress_sar_list(struct smr_ep *ep)
 				FI_WARN(&smr_prov, FI_LOG_EP_CTRL,
 					"unable to process rx completion\n");
 			}
-			dlist_remove(&sar_entry->entry);
 			if (sar_entry->rx_entry)
 				smr_get_peer_srx(ep)->owner_ops->free_entry(sar_entry->rx_entry);
+
+			ofi_ep_lock_acquire(&ep->util_ep);
+			dlist_remove(&sar_entry->entry);
 			ofi_freestack_push(ep->sar_fs, sar_entry);
+		} else {
+			ofi_ep_lock_acquire(&ep->util_ep);
+			sar_entry->in_use = false;
 		}
 	}
-	pthread_spin_unlock(&ep->region->lock);
+	ofi_ep_lock_release(&ep->util_ep);
 }
 
 void smr_ep_progress(struct util_ep *util_ep)
diff --git a/prov/shm/src/smr_rma.c b/prov/shm/src/smr_rma.c
index 0a6d5a91c..7aae23ae3 100644
--- a/prov/shm/src/smr_rma.c
+++ b/prov/shm/src/smr_rma.c
@@ -40,17 +40,11 @@
 
 
 static void smr_add_rma_cmd(struct smr_region *peer_smr,
-		const struct fi_rma_iov *rma_iov, size_t iov_count)
+		const struct fi_rma_iov *rma_iov, size_t iov_count,
+		struct smr_cmd_entry *cmd)
 {
-	struct smr_cmd *cmd;
-
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
-
-	cmd->rma.rma_count = iov_count;
-	memcpy(cmd->rma.rma_iov, rma_iov, sizeof(*rma_iov) * iov_count);
-
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
+	cmd->rma_cmd.rma.rma_count = iov_count;
+	memcpy(cmd->rma_cmd.rma.rma_iov, rma_iov, sizeof(*rma_iov) * iov_count);
 }
 
 static void smr_format_rma_resp(struct smr_cmd *cmd, fi_addr_t peer_id,
@@ -67,9 +61,10 @@ static ssize_t smr_rma_fast(struct smr_region *peer_smr, const struct iovec *iov
 			uint32_t op, uint64_t op_flags)
 {
 	struct iovec cma_iovec[SMR_IOV_LIMIT], rma_iovec[SMR_IOV_LIMIT];
-	struct smr_cmd *cmd;
+	struct smr_cmd_entry *cmd;
 	size_t total_len;
 	int ret, i;
+	int64_t pos;
 
 	memcpy(cma_iovec, iov, sizeof(*iov) * iov_count);
 	for (i = 0; i < rma_count; i++) {
@@ -85,12 +80,11 @@ static ssize_t smr_rma_fast(struct smr_region *peer_smr, const struct iovec *iov
 	if (ret)
 		return ret;
 
-	cmd = ofi_cirque_next(smr_cmd_queue(peer_smr));
-	smr_format_rma_resp(cmd, peer_id, rma_iov, rma_count, total_len,
+	cmd = smr_get_cmd(peer_smr, &pos);
+	smr_format_rma_resp(&cmd->cmd, peer_id, rma_iov, rma_count, total_len,
 			    (op == ofi_op_write) ? ofi_op_write_async :
 			    ofi_op_read_async, op_flags);
-	ofi_cirque_commit(smr_cmd_queue(peer_smr));
-	peer_smr->cmd_cnt--;
+	smr_queue_cmd(cmd, pos);
 
 	return 0;
 }
@@ -109,6 +103,8 @@ static ssize_t smr_generic_rma(struct smr_ep *ep, const struct iovec *iov,
 	ssize_t ret = 0;
 	size_t total_len;
 	bool use_ipc;
+	struct smr_cmd_entry *cmd;
+	int64_t pos;
 
 	assert(iov_count <= SMR_IOV_LIMIT);
 	assert(rma_count <= SMR_IOV_LIMIT);
@@ -128,12 +124,9 @@ static ssize_t smr_generic_rma(struct smr_ep *ep, const struct iovec *iov,
 		    (FI_REMOTE_CQ_DATA | FI_DELIVERY_COMPLETE)) &&
 		     rma_count == 1 && smr_cma_enabled(ep, peer_smr));
 
-	pthread_spin_lock(&peer_smr->lock);
-	if (peer_smr->cmd_cnt < cmds ||
-	    smr_peer_data(ep->region)[id].sar_status) {
-		ret = -FI_EAGAIN;
-		goto unlock_region;
-	}
+	if (ofi_atomic_get64(&peer_smr->cmd_cnt) < cmds ||
+	    smr_peer_data(ep->region)[id].sar_status)
+		return -FI_EAGAIN;
 
 	ofi_spin_lock(&ep->tx_lock);
 
@@ -171,12 +164,17 @@ static ssize_t smr_generic_rma(struct smr_ep *ep, const struct iovec *iov,
 	proto = smr_select_proto(use_ipc, smr_cma_enabled(ep, peer_smr), iface,
 				 op, total_len, op_flags);
 
+	cmd = smr_get_cmd(peer_smr, &pos);
 	ret = smr_proto_ops[proto](ep, peer_smr, id, peer_id, op, 0, data, op_flags,
-				   iface, device, iov, iov_count, total_len, context);
-	if (ret)
+				   iface, device, iov, iov_count, total_len,
+				   context, &cmd->cmd);
+	if (ret) {
+		smr_discard_cmd(peer_smr, cmd, pos);
 		goto unlock_cq;
+	}
 
-	smr_add_rma_cmd(peer_smr, rma_iov, rma_count);
+	smr_add_rma_cmd(peer_smr, rma_iov, rma_count, cmd);
+	smr_queue_cmd(cmd, pos);
 
 	if (proto != smr_src_inline && proto != smr_src_inject)
 		goto signal;
@@ -191,8 +189,6 @@ signal:
 	smr_signal(peer_smr);
 unlock_cq:
 	ofi_spin_unlock(&ep->tx_lock);
-unlock_region:
-	pthread_spin_unlock(&peer_smr->lock);
 	return ret;
 }
 
@@ -315,6 +311,8 @@ static ssize_t smr_generic_rma_inject(struct fid_ep *ep_fid, const void *buf,
 	int64_t id, peer_id;
 	int cmds, proto = smr_src_inline;
 	ssize_t ret = 0;
+	struct smr_cmd_entry *cmd;
+	int64_t pos;
 
 	assert(len <= SMR_INJECT_SIZE);
 	ep = container_of(ep_fid, struct smr_ep, util_ep.ep_fid.fid);
@@ -330,12 +328,9 @@ static ssize_t smr_generic_rma_inject(struct fid_ep *ep_fid, const void *buf,
 	cmds = 1 + !(domain->fast_rma && !(flags & FI_REMOTE_CQ_DATA) &&
 		     smr_cma_enabled(ep, peer_smr));
 
-	pthread_spin_lock(&peer_smr->lock);
-	if (peer_smr->cmd_cnt < cmds ||
-	    smr_peer_data(ep->region)[id].sar_status) {
-		ret = -FI_EAGAIN;
-		goto unlock_region;
-	}
+	if (ofi_atomic_get64(&peer_smr->cmd_cnt) < cmds ||
+	    smr_peer_data(ep->region)[id].sar_status)
+		return -FI_EAGAIN;
 
 	iov.iov_base = (void *) buf;
 	iov.iov_len = len;
@@ -347,21 +342,22 @@ static ssize_t smr_generic_rma_inject(struct fid_ep *ep_fid, const void *buf,
 		ret = smr_rma_fast(peer_smr, &iov, 1, &rma_iov, 1, NULL,
 				   peer_id, NULL, ofi_op_write, flags);
 		if (ret)
-			goto unlock_region;
+			return ret;
 		goto signal;
 	}
 
+	cmd = smr_get_cmd(peer_smr, &pos);
 	proto = len <= SMR_MSG_DATA_LEN ? smr_src_inline : smr_src_inject;
 	ret = smr_proto_ops[proto](ep, peer_smr, id, peer_id, ofi_op_write, 0,
-			data, flags, FI_HMEM_SYSTEM, 0, &iov, 1, len, NULL);
+			data, flags, FI_HMEM_SYSTEM, 0, &iov, 1, len,
+			NULL, &cmd->cmd);
 
 	assert(!ret);
-	smr_add_rma_cmd(peer_smr, &rma_iov, 1);
+	smr_add_rma_cmd(peer_smr, &rma_iov, 1, cmd);
+	smr_queue_cmd(cmd, pos);
 signal:
 	smr_signal(peer_smr);
 	ofi_ep_tx_cntr_inc_func(&ep->util_ep, ofi_op_write);
-unlock_region:
-	pthread_spin_unlock(&peer_smr->lock);
 	return ret;
 }
 
diff --git a/prov/util/src/util_shm.c b/prov/util/src/util_shm.c
index e0fd35300..98cb6a062 100644
--- a/prov/util/src/util_shm.c
+++ b/prov/util/src/util_shm.c
@@ -107,11 +107,12 @@ size_t smr_calculate_size_offsets(size_t tx_count, size_t rx_count,
 	rx_size = roundup_power_of_two(rx_count);
 
 	/* Align cmd_queue offset to 128-bit boundary. */
-	cmd_queue_offset = ofi_get_aligned_size(sizeof(struct smr_region), 16);
-	resp_queue_offset = cmd_queue_offset + sizeof(struct smr_cmd_queue) +
-			    sizeof(struct smr_cmd) * rx_size;
-	inject_pool_offset = resp_queue_offset + sizeof(struct smr_resp_queue) +
+	resp_queue_offset = ofi_get_aligned_size(sizeof(struct smr_region), 16);
+	cmd_queue_offset = resp_queue_offset + sizeof(struct smr_resp_queue) +
 			     sizeof(struct smr_resp) * tx_size;
+	inject_pool_offset = cmd_queue_offset +
+				sizeof(struct smr_cmd_queue) +
+				sizeof(struct smr_cmd_queue_entry) * rx_size;
 	sar_pool_offset = inject_pool_offset +
 		freestack_size(sizeof(struct smr_inject_buf), rx_size);
 	peer_data_offset = sar_pool_offset +
@@ -280,13 +281,11 @@ int smr_create(const struct fi_provider *prov, struct smr_map *map,
 	(*smr)->peer_data_offset = peer_data_offset;
 	(*smr)->name_offset = name_offset;
 	(*smr)->sock_name_offset = sock_name_offset;
-	(*smr)->cmd_cnt = rx_size;
-	/* Limit of 1 outstanding SAR message per peer */
-	(*smr)->sar_cnt = SMR_MAX_PEERS;
+	ofi_atomic_initialize64(&(*smr)->cmd_cnt, rx_size);
 	(*smr)->max_sar_buf_per_peer = SMR_BUF_BATCH_MAX;
 
-	smr_cmd_queue_init(smr_cmd_queue(*smr), rx_size);
 	smr_resp_queue_init(smr_resp_queue(*smr), tx_size);
+	smr_cmd_queue_init(smr_cmd_queue(*smr), rx_size);
 	smr_freestack_init(smr_inject_pool(*smr), rx_size,
 			sizeof(struct smr_inject_buf));
 	smr_freestack_init(smr_sar_pool(*smr), SMR_MAX_PEERS,

